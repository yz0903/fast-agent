import os

from fast_agent.llm.model_database import ModelDatabase
from fast_agent.llm.provider.openai.llm_openai import OpenAILLM
from fast_agent.llm.provider_types import Provider
from fast_agent.types import RequestParams

DEFAULT_OLLAMA_BASE_URL = "http://localhost:11434/v1"
DEFAULT_OLLAMA_MODEL = "llama3.2:latest"
DEFAULT_OLLAMA_API_KEY = "ollama"


class GenericLLM(OpenAILLM):
    def __init__(self, **kwargs) -> None:
        kwargs.pop("provider", None)
        super().__init__(provider=Provider.GENERIC, **kwargs)

    def _initialize_default_params(self, kwargs: dict) -> RequestParams:
        """Initialize Generic  parameters"""
        chosen_model = kwargs.get("model", DEFAULT_OLLAMA_MODEL)
        # Get model-aware max tokens, default to 8192 for Ollama models
        max_tokens = ModelDatabase.get_default_max_tokens(chosen_model) if chosen_model else 8192
        if max_tokens == 2048:  # If it's the fallback value, use a higher default for Ollama
            max_tokens = 8192

        return RequestParams(
            model=chosen_model,
            systemPrompt=self.instruction,
            parallel_tool_calls=True,
            max_iterations=10,
            maxTokens=max_tokens,
            use_history=True,
        )

    def _base_url(self) -> str | None:
        base_url: str | None = os.getenv("GENERIC_BASE_URL", DEFAULT_OLLAMA_BASE_URL)
        if self.context.config and self.context.config.generic:
            base_url = self.context.config.generic.base_url

        return base_url
